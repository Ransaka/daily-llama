from pathlib import Path
import numpy as np
import torch
import logging
import pandas as pd
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
from datasets import Dataset

logger = logging.getLogger()
logger.setLevel(logging.INFO)


class DailyLlamaVectorizer:
    def __init__(self, file_path, column_to_embed, content_column, model_id="intfloat/e5-small-v2"):
        self.df = pd.read_json(file_path).dropna(subset=[column_to_embed])
        self.model_id = model_id
        self.device = torch.device(
            "cuda:0" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModel.from_pretrained(self.model_id).to(self.device)
        self.column_to_embed = column_to_embed
        self.content_column = content_column
        self.content = self.df[self.content_column].values
        self.titles = self.df[self.column_to_embed].values

    def tokenize(self, batch):
        # Retrieve the column to embed from the batch
        column_to_embed = batch[self.column_to_embed]

        # Tokenize the text in the column using the tokenizer
        tokenized_text = self.tokenizer(
            column_to_embed, max_length=512, padding=True, truncation=True)

        # Return the tokenized text
        return tokenized_text

    def encode_single(self, text):
        # Tokenize the input text using the model's tokenizer
        inputs = self.tokenizer(text, max_length=512,
                                padding=True, truncation=True)

        # Filter out any inputs that are not expected by the model
        inputs = {k: torch.tensor([v], device=self.device) for k, v in inputs.items(
        ) if k in self.tokenizer.model_input_names}

        # Disable gradient calculation to save memory and computation
        with torch.no_grad():
            # Pass the tokenized inputs to the model and get the outputs
            outputs = self.model(**inputs)

        # Compute the average pooling of the last hidden state using attention mask
        embeddings = self.average_pool(
            outputs.last_hidden_state, inputs['attention_mask'])

        # Return the embeddings of the input text as a dictionary
        return {"embeddings": embeddings}

    def average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
        # Mask the last hidden states based on the attention mask
        last_hidden = last_hidden_states.masked_fill(
            ~attention_mask[..., None].bool(), 0.0)

        # Compute the sum of the masked last hidden states along the sequence length dimension
        sum_hidden = last_hidden.sum(dim=1)

        # Compute the sum of the attention mask values along the sequence length dimension
        sum_mask = attention_mask.sum(dim=1)

        # Divide the sum of the hidden states by the sum of the attention mask values
        # and add a new singleton dimension to match the dimensions of the hidden states
        avg_pool = sum_hidden / sum_mask[..., None]

        # Return the result of the average pooling operation
        return avg_pool

    def _get_embedding(self, batch):
        """
        Generate the embedding for a batch of inputs.

        Parameters:
            batch (dict): A dictionary containing the input data batch. The keys
                should correspond to the model's input names and the values should
                be tensors.

        Returns:
            dict: A dictionary containing the embeddings generated by the model.
                The key 'embeddings' maps to the tensor of embeddings.
        """

        # Move the batch tensors to the device
        inputs = {k: v.to(self.device) for k, v in batch.items()
                  if k in self.tokenizer.model_input_names}

        # Disable gradient calculations
        with torch.no_grad():
            # Forward pass through the model
            outputs = self.model(**inputs)

        # Average pool the last hidden state using attention mask
        embeddings = self.average_pool(
            outputs.last_hidden_state, inputs['attention_mask'])

        # Return the embeddings as a dictionary
        return {"embeddings": embeddings}

    def prepair_dataset(self):
        """
        Prepares the dataset for training.

        Returns:
            None
        """

        # Create a Dataset object from a pandas DataFrame with a single column
        data = Dataset.from_pandas(self.df[self.column_to_embed].to_frame())

        # Tokenize the dataset in batches using the provided tokenize function
        # The dataset is tokenized in parallel using multiple processes
        self.encoded_dataset = data.map(
            self.tokenize, batched=True, batch_size=None)

        # Set the format of the encoded dataset to be compatible with the torch library
        # The columns "input_ids" and "attention_mask" are selected for further processing
        self.encoded_dataset.set_format(
            "torch", columns=["input_ids", "attention_mask"])

    def run(self):
        # Prepare the dataset
        self.prepair_dataset()

        # Map the encoded dataset to embeddings using the _get_embedding function
        dataset_hidden = self.encoded_dataset.map(
            self._get_embedding, batched=True, batch_size=100)

        # Detach the embeddings tensor from the computation graph, move it to the CPU, and convert it to a numpy array
        self.embeddings = dataset_hidden['embeddings'].detach().cpu().numpy()

    def retrave_embeddings(self, out_path=None, output_type="pandas"):
        # Run the function
        self.run()

        if output_type == "pandas":
            # Create a DataFrame with the embeddings array and columns labeled as '0', '1', '2', ...
            embeddings_df = pd.DataFrame(self.embeddings, columns=[
                                         str(i) for i in range(self.embeddings.shape[1])])

            # Concatenate the embeddings DataFrame with the original DataFrame
            embeddings_df = pd.concat([embeddings_df, self.df], axis=1)

            # Log the shape of the final embeddings DataFrame
            logger.info(f"Final embeddings with size: {embeddings_df.shape}")

            if out_path:
                # If out_path is provided, save the embeddings DataFrame as a feather file
                out_path = Path(out_path)
                embeddings_df.to_feather(out_path / 'embeddings.feather')

            # Return the embeddings DataFrame
            return embeddings_df

        elif output_type == "numpy":
            if out_path:
                # If out_path is provided, save the embeddings array as a binary file
                out_path = Path(out_path)
                np.save(out_path / 'embeddings.bin', self.embeddings)

            # Return the embeddings array and the values of the specified column in the original DataFrame
            return self.embeddings
